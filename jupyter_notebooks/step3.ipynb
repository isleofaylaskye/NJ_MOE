{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# Define input and output file paths\n",
    "path = os.path.join(os.path.dirname(os.getcwd()))\n",
    "path='/Users/ayla/workflow'\n",
    "ipath = os.path.join(path,'data')\n",
    "opath = os.path.join(path,'data')\n",
    "input_file_dir=os.path.join(ipath)\n",
    "output_file_dir=os.path.join(opath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the ipath and opath are identical. This is because this script uses one file called \"data\" for all inputs and outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Clean the Data\n",
    "The VRE table will include some BGs that have zero population. We must remove these zero population BGs from our csv file. \n",
    "\n",
    "Before we do this, we should identify the number of BGs that have a non-zero population but have zero population 2.0 times the poverty line (this implies that the entire population is low income).\n",
    "We donâ€™t want to accidentally delete BGs that are 100% low income. The script below identifies 100% low income BGs. It also retruns the total number of rows in the input file. This will be relevant later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of block groups in the input file: 6599\n",
      "Number of 100% low income block groups\n",
      "11.0\n",
      "                      GEOID  ORDER  ESTIMATE  Var_Rep1  Var_Rep2  Var_Rep3  \\\n",
      "3260  1500000US340076105001      1       147       109       135       186   \n",
      "3261  1500000US340076105001      8         0         0         0         0   \n",
      "3326  1500000US340076115003      1        78        99        79       106   \n",
      "3327  1500000US340076115003      8         0         0         0         0   \n",
      "5104  1500000US340139801001      1         4         4         4         5   \n",
      "5105  1500000US340139801001      8         0         0         0         0   \n",
      "5138  1500000US340155002043      1        54        40        48        21   \n",
      "5139  1500000US340155002043      8         0         0         0         0   \n",
      "5496  1500000US340155020026      1        11        10         1        24   \n",
      "5497  1500000US340155020026      8         0         0         0         0   \n",
      "5522  1500000US340155024001      1         4        15         0         1   \n",
      "5523  1500000US340155024001      8         0         0         0         0   \n",
      "5846  1500000US340170067002      1       160       157       214       140   \n",
      "5847  1500000US340170067002      8         0         0         0         0   \n",
      "6660  1500000US340210004002      1       395       213       514       261   \n",
      "6661  1500000US340210004002      8         0         0         0         0   \n",
      "6722  1500000US340210013001      1       294       292       303       308   \n",
      "6723  1500000US340210013001      8         0         0         0         0   \n",
      "6984  1500000US340210037072      1        14         5        14        14   \n",
      "6985  1500000US340210037072      8         0         0         0         0   \n",
      "7166  1500000US340210045011      1        54        53        53        56   \n",
      "7167  1500000US340210045011      8         0         0         0         0   \n",
      "\n",
      "      Var_Rep4  Var_Rep5  Var_Rep6  Var_Rep7  ...  Var_Rep71  Var_Rep72  \\\n",
      "3260       147       107        83       138  ...        133        178   \n",
      "3261         0         0         0         0  ...          0          0   \n",
      "3326        75        81        50        69  ...         94         52   \n",
      "3327         0         0         0         0  ...          0          0   \n",
      "5104         4         4         4         4  ...          4          4   \n",
      "5105         0         0         0         0  ...          0          0   \n",
      "5138       102        53        43        61  ...         74         21   \n",
      "5139         0         0         0         0  ...          0          0   \n",
      "5496        15        10         9        17  ...         22         20   \n",
      "5497         0         0         0         0  ...          0          0   \n",
      "5522        10         1         4         5  ...          8          4   \n",
      "5523         0         0         0         0  ...          0          0   \n",
      "5846       185       176       100       261  ...         95         85   \n",
      "5847         0         0         0         0  ...          0          0   \n",
      "6660       448       568       416       469  ...        260        403   \n",
      "6661         0         0         0         0  ...          0          0   \n",
      "6722       275       278       286       291  ...        278        262   \n",
      "6723         0         0         0         0  ...          0          0   \n",
      "6984        15        21        24        13  ...         13          5   \n",
      "6985         0         0         0         0  ...          0          0   \n",
      "7166        22        47        53        92  ...         23         67   \n",
      "7167         0         0         0         0  ...          0          0   \n",
      "\n",
      "      Var_Rep73  Var_Rep74  Var_Rep75  Var_Rep76  Var_Rep77  Var_Rep78  \\\n",
      "3260        166        118        212        188        195        107   \n",
      "3261          0          0          0          0          0          0   \n",
      "3326         65         98         99         76         83         85   \n",
      "3327          0          0          0          0          0          0   \n",
      "5104          4          5          4          4          4          5   \n",
      "5105          0          0          0          0          0          0   \n",
      "5138         57         19         19         63         79         55   \n",
      "5139          0          0          0          0          0          0   \n",
      "5496         15         11         22          1         10          1   \n",
      "5497          0          0          0          0          0          0   \n",
      "5522          4          0          3          6         13          4   \n",
      "5523          0          0          0          0          0          0   \n",
      "5846        138        170        248        290        123        158   \n",
      "5847          0          0          0          0          0          0   \n",
      "6660        422        435        373        425        616        407   \n",
      "6661          0          0          0          0          0          0   \n",
      "6722        284        280        288        292        288        288   \n",
      "6723          0          0          0          0          0          0   \n",
      "6984         12         21         22         19          4          4   \n",
      "6985          0          0          0          0          0          0   \n",
      "7166         65         20         29         81         53         65   \n",
      "7167          0          0          0          0          0          0   \n",
      "\n",
      "      Var_Rep79  Var_Rep80  \n",
      "3260        195        112  \n",
      "3261          0          0  \n",
      "3326         85         86  \n",
      "3327          0          0  \n",
      "5104          4          5  \n",
      "5105          0          0  \n",
      "5138         42         77  \n",
      "5139          0          0  \n",
      "5496          1         16  \n",
      "5497          0          0  \n",
      "5522          2          3  \n",
      "5523          0          0  \n",
      "5846        154        105  \n",
      "5847          0          0  \n",
      "6660        212        213  \n",
      "6661          0          0  \n",
      "6722        315        287  \n",
      "6723          0          0  \n",
      "6984         28         25  \n",
      "6985          0          0  \n",
      "7166         67         62  \n",
      "7167          0          0  \n",
      "\n",
      "[22 rows x 83 columns]\n",
      "Results have been written to /Users/ayla/workflow/data/100%_low_income_bgs.csv\n"
     ]
    }
   ],
   "source": [
    "#Identifies row pairs for which both order = 8 has a corresponding estimate that equals zero (indicating zero population 2.0 times poverty line) \n",
    "#AND order = 1 has a non-zero estimate (has a total population)\n",
    "#If a row pair meets these requirements, it means it is a 100% low income block group\n",
    "import pandas as pd\n",
    "#'/Users/ayla/workflow/data/edited_VRE_complete_NJ.csv'\n",
    "\n",
    "# Load the CSV file\n",
    "input_file = f\"{ipath}/{'edited_VRE_complete_NJ.csv'}\"  # Replace with your CSV file name\n",
    "data = pd.read_csv(input_file)\n",
    "\n",
    "# Calculate the number of block groups in the input file\n",
    "num_original = data.shape[0] // 2\n",
    "print(\"Number of block groups in the input file:\", num_original)\n",
    "\n",
    "# Initialize a list to store the identified pairs\n",
    "identified_pairs = []\n",
    "\n",
    "# Iterate over the dataframe, processing each pair of rows\n",
    "for i in range(1, data.shape[0], 2):\n",
    "    if data.iloc[i-1, 1] == 1 and data.iloc[i, 1] == 8 and data.iloc[i-1, 0] == data.iloc[i, 0]:\n",
    "        # Extract the GEOID for the current pair\n",
    "        geoid = data.iloc[i-1, 0]\n",
    "        \n",
    "        # Extract the estimates for the current pair\n",
    "        estimate_order_1 = data.iloc[i-1]['ESTIMATE']\n",
    "        estimate_order_8 = data.iloc[i]['ESTIMATE']\n",
    "        \n",
    "        # Check if ORDER = 8 estimate is zero and ORDER = 1 estimate is non-zero\n",
    "        if estimate_order_8 == 0 and estimate_order_1 != 0:\n",
    "            # Append the rows to the identified_pairs list\n",
    "            identified_pairs.append(data.iloc[i-1, :])\n",
    "            identified_pairs.append(data.iloc[i, :])\n",
    "\n",
    "# Convert the list of identified pairs to a DataFrame for easier visualization\n",
    "all_low_income_bg_df = pd.DataFrame(identified_pairs)\n",
    "\n",
    "#define df as data frame of identified pairs only\n",
    "df = pd.DataFrame(identified_pairs)\n",
    "\n",
    "#Print number of identified pairs\n",
    "num_rows = df.shape[0]\n",
    "print(\"Number of 100% low income block groups\")\n",
    "#print \"num_rows\" divided by two since each pair of rows represents 1 block group\n",
    "print(num_rows/2)\n",
    "#store this value for later \n",
    "num_all_low_income = (num_rows/2)\n",
    "# Print the identified pairs\n",
    "print(all_low_income_bg_df)\n",
    "\n",
    "# Define the output file path\n",
    "output_file = f\"{opath}/100%_low_income_bgs.csv\"\n",
    "\n",
    "# Write the identified pairs to a CSV file\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Results have been written to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's consider the BGs that have zero total population. We know logically that these BGs should also contain zero individuals 2.0 times the poverty line. \n",
    "\n",
    "However, we must test this assumption to ensure the accuracy and validity of our data. In order to perform this test, run the following script that tests for BG pairs in which total population is zero but percent 2.0 times the poverty line is non-zero. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row pairs for which total population estimate is zero and population > 2.0 poverty line estimate is non-zero:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#order = 1 (total population) has an estimate that equals zero, but order = 8 (population > 2.0 times the poverty line) has a non-zero estimate\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file (same input file as previous code box)\n",
    "input_file = f\"{ipath}/{'edited_VRE_complete_NJ.csv'}\"  # Replace with your CSV file name\n",
    "data = pd.read_csv(input_file)\n",
    "\n",
    "# Initialize a list to store the identified pairs\n",
    "identified_pairs = []\n",
    "\n",
    "# Iterate over the dataframe, processing each pair of rows\n",
    "for i in range(1, data.shape[0], 2):\n",
    "    if data.iloc[i-1, 1] == 1 and data.iloc[i, 1] == 8 and data.iloc[i-1, 0] == data.iloc[i, 0]:\n",
    "        # Extract the GEOID for the current pair\n",
    "        geoid = data.iloc[i-1, 0]\n",
    "        \n",
    "        # Extract the estimates for the current pair\n",
    "        estimate_order_1 = data.iloc[i-1]['ESTIMATE']\n",
    "        estimate_order_8 = data.iloc[i]['ESTIMATE']\n",
    "        \n",
    "        # Check if ORDER = 1 estimate is zero and ORDER = 8 estimate is non-zero\n",
    "        if estimate_order_1 == 0 and estimate_order_8 != 0:\n",
    "            # Append the rows to the identified_pairs list\n",
    "            identified_pairs.append(data.iloc[i-1, :5])\n",
    "            identified_pairs.append(data.iloc[i, :5])\n",
    "\n",
    "# Convert the list of identified pairs to a DataFrame for easier visualization\n",
    "zero_pop_nonzero_est_df = pd.DataFrame(identified_pairs)\n",
    "\n",
    "# Print the identified pairs\n",
    "print(\"Row pairs for which total population estimate is zero and population > 2.0 poverty line estimate is non-zero:\")\n",
    "print(zero_pop_nonzero_est_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script should return an empty data frame. This indicates that there are no row pairs for which BG pairs in which total population is zero but percent 2.0 times the poverty line is non-zero. If it is not empty, ensure that the correct file is being used as the input file. If it is empty, please continue.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now remove all of the zero population BGs from our csv file. Let's start by printing them and saving them to a seperate csv file called \"deleted_zero_pop_bgs.csv\" in case we want to reference them again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of zero population block groups\n",
      "90\n",
      "                       GEOID  ORDER  ESTIMATE  Var_Rep1  Var_Rep2  Var_Rep3  \\\n",
      "386    1500000US340019900000      1         0         0         0         0   \n",
      "387    1500000US340019900000      8         0         0         0         0   \n",
      "1974   1500000US340057001043      1         0         0         0         0   \n",
      "1975   1500000US340057001043      8         0         0         0         0   \n",
      "1986   1500000US340057003034      1         0         0         0         0   \n",
      "...                      ...    ...       ...       ...       ...       ...   \n",
      "10725  1500000US340299801001      8         0         0         0         0   \n",
      "10726  1500000US340299900000      1         0         0         0         0   \n",
      "10727  1500000US340299900000      8         0         0         0         0   \n",
      "11576  1500000US340339900000      1         0         0         0         0   \n",
      "11577  1500000US340339900000      8         0         0         0         0   \n",
      "\n",
      "       Var_Rep4  Var_Rep5  Var_Rep6  Var_Rep7  ...  Var_Rep71  Var_Rep72  \\\n",
      "386           0         0         0         0  ...          0          0   \n",
      "387           0         0         0         0  ...          0          0   \n",
      "1974          0         0         0         0  ...          0          0   \n",
      "1975          0         0         0         0  ...          0          0   \n",
      "1986          0         0         0         0  ...          0          0   \n",
      "...         ...       ...       ...       ...  ...        ...        ...   \n",
      "10725         0         0         0         0  ...          0          0   \n",
      "10726         0         0         0         0  ...          0          0   \n",
      "10727         0         0         0         0  ...          0          0   \n",
      "11576         0         0         0         0  ...          0          0   \n",
      "11577         0         0         0         0  ...          0          0   \n",
      "\n",
      "       Var_Rep73  Var_Rep74  Var_Rep75  Var_Rep76  Var_Rep77  Var_Rep78  \\\n",
      "386            0          0          0          0          0          0   \n",
      "387            0          0          0          0          0          0   \n",
      "1974           0          0          0          0          0          0   \n",
      "1975           0          0          0          0          0          0   \n",
      "1986           0          0          0          0          0          0   \n",
      "...          ...        ...        ...        ...        ...        ...   \n",
      "10725          0          0          0          0          0          0   \n",
      "10726          0          0          0          0          0          0   \n",
      "10727          0          0          0          0          0          0   \n",
      "11576          0          0          0          0          0          0   \n",
      "11577          0          0          0          0          0          0   \n",
      "\n",
      "       Var_Rep79  Var_Rep80  \n",
      "386            0          0  \n",
      "387            0          0  \n",
      "1974           0          0  \n",
      "1975           0          0  \n",
      "1986           0          0  \n",
      "...          ...        ...  \n",
      "10725          0          0  \n",
      "10726          0          0  \n",
      "10727          0          0  \n",
      "11576          0          0  \n",
      "11577          0          0  \n",
      "\n",
      "[180 rows x 83 columns]\n",
      "Results have been written to /Users/ayla/workflow/data/deleted_zero_pop_bgs.csv\n"
     ]
    }
   ],
   "source": [
    "#order = 1 (total population) has an estimate that equals zero, AND order = 8 (population > 2.0 times the poverty line) estimate equals zero\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file (same input file as previous code box)\n",
    "input_file = f\"{ipath}/{'edited_VRE_complete_NJ.csv'}\"  # Replace with your CSV file name\n",
    "data = pd.read_csv(input_file)\n",
    "\n",
    "# Load the CSV file\n",
    "data = pd.read_csv(input_file)\n",
    "\n",
    "# Initialize a list to store the identified pairs\n",
    "identified_pairs = []\n",
    "\n",
    "# Iterate over the dataframe, processing each pair of rows\n",
    "for i in range(1, data.shape[0], 2):\n",
    "    if data.iloc[i-1, 1] == 1 and data.iloc[i, 1] == 8 and data.iloc[i-1, 0] == data.iloc[i, 0]:\n",
    "        # Extract the GEOID for the current pair\n",
    "        geoid = data.iloc[i-1, 0]\n",
    "        \n",
    "        # Extract the estimates for the current pair\n",
    "        estimate_order_1 = data.iloc[i-1]['ESTIMATE']\n",
    "        estimate_order_8 = data.iloc[i]['ESTIMATE']\n",
    "        \n",
    "        # Check if both ORDER = 1 and ORDER = 8 estimates are zero\n",
    "        if estimate_order_1 == 0 and estimate_order_8 == 0:\n",
    "            # Append the rows to the identified_pairs list\n",
    "            identified_pairs.append(data.iloc[i-1, :])\n",
    "            identified_pairs.append(data.iloc[i, :])\n",
    "\n",
    "# Convert the list of identified pairs to a DataFrame for easier visualization\n",
    "zero_pop_df = pd.DataFrame(identified_pairs)\n",
    "\n",
    "# Define df as data frame of identified pairs only\n",
    "df = zero_pop_df\n",
    "\n",
    "# Print number of identified pairs\n",
    "num_rows = df.shape[0]\n",
    "print(\"Number of zero population block groups\")\n",
    "\n",
    "# Print \"num_rows\" divided by two since each pair of rows represents 1 block group\n",
    "print(num_rows // 2)\n",
    "\n",
    "# Store this value for later use\n",
    "num_zero_pop = num_rows // 2\n",
    "\n",
    "# Print the identified pairs\n",
    "print(zero_pop_df)\n",
    "\n",
    "# Define the output file path\n",
    "output_file = f\"{opath}/deleted_zero_pop_bgs.csv\"\n",
    "\n",
    "# Write the identified pairs to a CSV file\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Results have been written to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now let's actually remove all of the zero population BGs from our csv file. The output of the following script is a new csv file titled \"filtered_complete_NJ.csv\" that contains all of the original BGs except for the zero population BGs identified above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of block groups in filtered_complete_NJ.csv 6509\n",
      "Filtered results have been written to /Users/ayla/workflow/data/filtered_complete_NJ.csv\n",
      "The number of block groups matches after subtraction.\n"
     ]
    }
   ],
   "source": [
    "#Create new csv file without zero pop BGs\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file (same input file as previous code box)\n",
    "input_file = f\"{ipath}/{'edited_VRE_complete_NJ.csv'}\"  # Replace with your CSV file name\n",
    "data = pd.read_csv(input_file)\n",
    "\n",
    "# Define the estimate column\n",
    "estimate_column = 'ESTIMATE'  # Replace with the actual column name if different\n",
    "\n",
    "# Identify rows with ORDER = 8 and Estimate = 0\n",
    "rows_with_order_8_zero = data[(data['ORDER'] == 8) & (data[estimate_column] == 0)]\n",
    "\n",
    "# Identify rows with ORDER = 1 and Estimate = 0\n",
    "rows_with_order_1_zero = data[(data['ORDER'] == 1) & (data[estimate_column] == 0)]\n",
    "\n",
    "# Get the block group identifiers for these rows\n",
    "block_group_order_8_zero = set(rows_with_order_8_zero['GEOID'])\n",
    "block_group_order_1_zero = set(rows_with_order_1_zero['GEOID'])\n",
    "\n",
    "# Find common block groups where both ORDER = 8 and ORDER = 1 have zero estimates\n",
    "common_block_groups = block_group_order_8_zero.intersection(block_group_order_1_zero)\n",
    "\n",
    "# Filter out rows with these common block groups\n",
    "filtered_data = data[~data['GEOID'].isin(common_block_groups)]\n",
    "\n",
    "# Define the output file path\n",
    "output_file = f\"{opath}/filtered_complete_NJ.csv\"\n",
    "\n",
    "# Write the filtered DataFrame to a new CSV file\n",
    "filtered_data.to_csv(output_file, index=False)\n",
    "\n",
    "data = pd.read_csv(output_file)\n",
    "\n",
    "# Calculate the number of block groups in the input file\n",
    "num_filtered_data = data.shape[0]//2\n",
    "print(\"Number of block groups in filtered_complete_NJ.csv\", num_filtered_data)\n",
    "\n",
    "print(f\"Filtered results have been written to {output_file}\")\n",
    "\n",
    "#Check to make sure that the number of block groups in the new csv file is equal to the original csv file - the number of zero pop block groups\n",
    "\n",
    "if num_filtered_data == num_original - num_zero_pop:\n",
    "    print(\"The number of block groups matches after subtraction.\")\n",
    "else:\n",
    "    print(\"There is a discrepancy in the number of block groups after subtraction.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now make sure that 100% low income block groups weren't deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All rows in the subset file are present in the main file.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "main_filename = \"filtered_complete_NJ.csv\"\n",
    "subset_filename = \"100%_low_income_bgs.csv\"\n",
    "\n",
    "# Load the CSV files\n",
    "main_file = pd.read_csv(f\"{ipath}/{main_filename}\")\n",
    "subset_file = pd.read_csv(f\"{ipath}/{subset_filename}\")\n",
    "\n",
    "# Check if all rows in the subset file are present in the main file\n",
    "merged = subset_file.merge(main_file, how='left', indicator=True)\n",
    "missing_rows = merged[merged['_merge'] == 'left_only']\n",
    "\n",
    "if missing_rows.empty:\n",
    "    print(\"All rows in the subset file are present in the main file.\")\n",
    "else:\n",
    "    print(\"Some rows in the subset file are not present in the main file.\")\n",
    "    print(\"The following rows in the subset file are not present in the main file:\")\n",
    "    print(missing_rows.drop(columns='_merge'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for zero values in the variance replicate estimates of the remaining \"total population\" rows. Zero values will cause NaN values in the later confidence interval script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows where Order = 1 and any column has a zero value:\n",
      "                      GEOID  ORDER  ESTIMATE  Var_Rep1  Var_Rep2  Var_Rep3  \\\n",
      "5404  1500000US340155020026      1        11        10         1        24   \n",
      "5430  1500000US340155024001      1         4        15         0         1   \n",
      "\n",
      "      Var_Rep4  Var_Rep5  Var_Rep6  Var_Rep7  ...  Var_Rep71  Var_Rep72  \\\n",
      "5404        15        10         9        17  ...         22         20   \n",
      "5430        10         1         4         5  ...          8          4   \n",
      "\n",
      "      Var_Rep73  Var_Rep74  Var_Rep75  Var_Rep76  Var_Rep77  Var_Rep78  \\\n",
      "5404         15         11         22          1         10          1   \n",
      "5430          4          0          3          6         13          4   \n",
      "\n",
      "      Var_Rep79  Var_Rep80  \n",
      "5404          1         16  \n",
      "5430          2          3  \n",
      "\n",
      "[2 rows x 83 columns]\n"
     ]
    }
   ],
   "source": [
    "#check for zero values in replicates of remaining total population rows\n",
    "import pandas as pd\n",
    "\n",
    "# Define the file path\n",
    "path = os.path.join(os.path.dirname(os.getcwd()))\n",
    "path='/Users/ayla/workflow'\n",
    "ipath = os.path.join(path,'data')\n",
    "filename = \"filtered_complete_NJ.csv\"\n",
    "csv_file_path = f\"{ipath}/{filename}\"\n",
    "\n",
    "# Load the CSV file\n",
    "data = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Filter rows where Order = 1\n",
    "order_1_rows = data[data['ORDER'] == 1]\n",
    "\n",
    "# Further filter rows where any column has a zero value\n",
    "rows_with_zero_values = order_1_rows[(order_1_rows == 0).any(axis=1)]\n",
    "\n",
    "# Print the identified rows\n",
    "print(\"Rows where Order = 1 and any column has a zero value:\")\n",
    "print(rows_with_zero_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
